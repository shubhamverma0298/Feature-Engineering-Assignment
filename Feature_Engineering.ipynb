{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering Assignment**"
      ],
      "metadata": {
        "id": "8NRucuiVYgTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 1 What is a parameter?**\n",
        "  - something that decides or limits the way in which something can be done\n",
        "  - In feature engineering, a parameter refers to a model-specific value or weight that is adjusted during the training process to improve the model's fit to the data.\n",
        "\n",
        "**Q 2 What is correlation? && What does negative correlation mean?**\n",
        "  - In feature engineering, correlation refers to a statistical measure that quantifies the relationship between two variables (features) in a dataset\n",
        "  - Negative correlation : Negative correlation also called inverse correlation, which is a relationship between two variables in which one increases as the other decreases, and vice versa.\n",
        "\n",
        "**Q 3 Define Machine Learning. What are the main components in Machine Learning?**\n",
        "  - Machine learning is a field of artificial intelligence (AI) that enables systems to learn from data and make predictions or decisions without being explicitly programmed.\n",
        "  - It involves training algorithms on data to identify patterns and make accurate predictions on new, unseen data.\n",
        "  - The main components of machine learning include:\n",
        "    - Data: The raw material for machine learning models, providing the information algorithms learn from.\n",
        "    - Algorithms:The sets of rules and statistical techniques used to analyze and interpret data.\n",
        "    - Models:The representation of learned patterns, often in the form of equations, decision trees, or neural networks.\n",
        "    - Predictions:The output of a model, providing insights or decisions based on learned patterns.\n",
        "   - Evaluation:Measuring the performance of a model, often using metrics like accuracy, precision, or recall.\n",
        "\n",
        "**Q 4 How does loss value help in determining whether the model is good or not?**\n",
        "  - A model's loss value, essentially a measure of its prediction accuracy, is a key indicator of its overall performance.\n",
        "  - A lower loss value generally indicates a better-performing model, meaning its predictions are closer to the actual values.\n",
        "\n",
        "**Q 5 What are continuous and categorical variables?**\n",
        "  - continuous variables : continious variables are numerical values with continuous form . like 1,2,3,4,5\n",
        "  - categorical variables : categorical variables are non numerical data grouped into categories or groups. like banana , mango ,apple\n",
        "\n",
        "**Q 6 How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "  - Categorical variables, which represent distinct categories, need to be transformed into a numerical format suitable for machine learning algorithms.\n",
        "  - Common techniques include :-\n",
        "      1. One-Hot Encoding\n",
        "      2. Label Encoding\n",
        "      3. Ordinal Encoding\n",
        "      4. Frequency Encoding\n",
        "      5. Target Encoding (Mean Encoding)\n",
        "      6. Binary Encoding\n",
        "\n",
        "**Q 7 What do you mean by training and testing a dataset?**\n",
        "  - Training Data:This subset of the dataset is used to \"train\" or \"learn\" the machine learning model. The model examines the training data to identify relationships between input features and output values, and it adjusts its parameters to minimize errors on the training data.\n",
        "  - Testing Data:Once the model is trained, the testing data is used to assess its performance. This data is different from the training data, ensuring that the model is evaluated on unseen examples. The model makes predictions on the testing data, and these predictions are compared to the actual values to determine the model's accuracy and generalization ability.\n",
        "\n",
        "**Q 8 What is sklearn.preprocessing?**\n",
        "  - sklearn.preprocessing :- sklearn.preprocessing is a module in the scikit-learn library in Python that provides functions and classes to preprocess data before training machine learning models.\n",
        "  - Preprocessing is a crucial step in machine learning as it transforms raw data into a suitable format that can improve the performance of models. It includes techniques for scaling, normalizing, encoding, and imputing data.\n",
        "\n",
        "**Q 9 What is a Test set?**\n",
        "  - A test set is a subset of data, separate from the training set, used to evaluate how well a model performs on unseen data after it has been trained.\n",
        "\n",
        "**Q 10 How do we split data for model fitting (training and testing) in Python?\n",
        "&& How do you approach a Machine Learning problem?**\n",
        "  \n",
        "  - In Python, data is typically split for training and testing using the train_test_split function from the scikit-learn library. This function randomly divides the data into two subsets: one for training the model and another for evaluating its performance on unseen data.\n",
        "  - A structured approach to a Machine Learning (ML) problem involves :- - -    \n",
        "   - Defining the problem.\n",
        "   - Collecting and preparing data.\n",
        "   - Selecting an appropriate model.\n",
        "   - Training it.\n",
        "   - Evaluating its performance.\n",
        "   - Potentially deploying and monitoring it.\n",
        "\n",
        "**Q 11 Why do we have to perform EDA before fitting a model to the data?**\n",
        "  - Exploratory Data Analysis (EDA) before model fitting is crucial for several reasons:-\n",
        "   - It allows data scientists to understand the characteristics of the data.\n",
        "   - Identify patterns.\n",
        "   - Detect potential issues before building a model.\n",
        "\n",
        "**Q 12 What is correlation?**\n",
        "  - In feature engineering, correlation refers to a statistical measure that quantifies the relationship between two variables (features) in a dataset\n",
        "\n",
        "**Q 13 What does negative correlation mean?**\n",
        "  - Negative correlation : Negative correlation also called inverse correlation, which is a relationship between two variables in which one increases as the other decreases, and vice versa.\n",
        "\n",
        "**Q 14 How can you find correlation between variables in Python?**\n",
        "  - We can find correlation between variables in Python by using pandas module\n",
        "  and then use the function corr().\n",
        "  - example :-\n",
        "     \n",
        "          import pandas as pd\n",
        "\n",
        "          data = {'a':[1,2,3,4],'b':[5,6,7,8]}\n",
        "\n",
        "          data_f=pd.DataFrame(data)\n",
        "\n",
        "          data_correlation = data_f.corr()\n",
        "\n",
        "          print(data_correlation)\n",
        "\n",
        "          output::\n",
        "\n",
        "          \t   a\t b\n",
        "            a\t1.0\t1.0\n",
        "            b\t1.0\t1.0\n",
        "\n",
        "\n",
        "**Q 15 What is causation? Explain difference between correlation and causation with an example?**\n",
        "  - Causation : A direct cause-and-effect relationship. If event A happens, and it directly causes event B to happen, that's causation.\n",
        "  - Correlation : Two events occurring together or showing a relationship, but not necessarily one causing the other.\n",
        "  - Example:\n",
        "      - Causation : You turn on the light switch (event A), and the light bulb turns on (event B). The switch directly causes the light to come on.\n",
        "      - Correlation : Ice cream sales increase during the summer, and shark attacks also increase during the summer. These are correlated because both events occur more frequently during the warm weather, but ice cream sales don't cause shark attacks. A more likely explanation is that both are linked to the higher number of people swimming in the ocean during summer.\n",
        "\n",
        "**Q 16 What is an Optimizer? What are different types of optimizers? Explain each with an example?**\n",
        "  - Optimizer :- optimizer is an algorithm used in deep learning to minimize the loss function during model training, adjusting the model's parameters (weights and biases) to improve accuracy.\n",
        "  - Types of optimizers are :-\n",
        "    - Gradient Descent (GD)\n",
        "    - Stochastic Gradient Descent (SGD)\n",
        "    - Mini-Batch SGD\n",
        "    - SGD with Momentum\n",
        "    - Adaptive Gradient (AdaGrad)\n",
        "    - Root Mean Squared Propagation (RMSProp)\n",
        "    - Adaptive Moment Estimation (Adam)\n",
        "\n",
        "  - Gradient Descent (GD) : A basic optimization algorithm that iteratively updates parameters in the opposite direction of the gradient of the loss function. Example: Imagine a ball rolling down a hill (loss function); GD moves the ball in the direction of steepest descent to find the lowest point (minimum loss).\n",
        "  - Stochastic Gradient Descent (SGD) : An improvement over GD that uses only a single training example per iteration to update the parameters. This makes it faster but potentially noisier. Example: Instead of using all the data points to find the slope (gradient), SGD uses only one point at a time, leading to faster, but sometimes less stable, convergence.\n",
        "  - Mini-Batch SGD : A balance between SGD and batch gradient descent. It uses a small batch of training examples per iteration, providing a more stable and efficient update than SGD. Example: Using a group of 32 images to calculate the gradient, rather than one image or the entire dataset, offers a good trade-off between speed and stability.\n",
        "  - SGD with Momentum : Adds a \"momentum\" term to SGD, which simulates inertia. This helps the algorithm escape local minima and converge faster. Example: Think of a ball rolling down a hill; momentum allows it to continue moving in the same direction, even if the gradient changes slightly, preventing it from getting stuck in a shallow valley.\n",
        "  - Adaptive Gradient (AdaGrad) : Dynamically adjusts the learning rate for each parameter based on its historical gradient. This is useful for sparse data where some parameters might not be updated as frequently. Example: If one parameter is updated frequently, AdaGrad might reduce its learning rate to avoid overshooting. If another parameter is updated less frequently, it might increase its learning rate to ensure it's updated appropriately.\n",
        "  - Root Mean Squared Propagation (RMSProp) : Another adaptive learning rate optimizer that uses a moving average of the squared gradients to update parameters. Example: RMSProp keeps track of the average magnitude of the gradients over time, allowing the algorithm to adapt to changing landscape.\n",
        "  - Adaptive Moment Estimation (Adam) : Combines momentum with adaptive learning rates, offering a robust and efficient optimization algorithm. Example: Adam uses a moving average of both the gradients and their squares, similar to RMSProp, but also includes a momentum term, making it generally faster and more stable.\n",
        "\n",
        "**Q 17 What is sklearn.linear_model ?**\n",
        "  - sklearn.linear_model is a module in the scikit-learn (sklearn) library that implements various linear models for regression and classification tasks. Linear models predict the target variable as a linear combination of the - - - input features. It includes algorithms like:\n",
        "    - Linear Regression: Predicts a continuous target variable.\n",
        "    - Logistic Regression: Predicts the probability of a binary outcome.\n",
        "    - Ridge Regression: Adds a penalty to the coefficients to prevent overfitting.\n",
        "    - Lasso Regression: Sets some coefficients to zero for feature selection.\n",
        "    - Elastic Net: Combines Ridge and Lasso penalties.\n",
        "    - Perceptron: A simple linear classifier.\n",
        "    - SGDClassifier/Regressor: Linear models trained with stochastic gradient descent.\n",
        "  \n",
        "**Q 18 What does model.fit() do? What arguments must be given?**\n",
        "  - The model.fit() function in machine learning frameworks like TensorFlow and Keras trains a model by adjusting its internal parameters (weights and biases) to minimize a loss function. This process involves iterating over the provided training data for a specified number of epochs.\n",
        "  - The required arguments for model.fit() are:\n",
        "     - x : Training data. It can be a NumPy array, a list of arrays (for multi-input models), a dictionary mapping input names to arrays (for named inputs), or a dataset object.\n",
        "     - y : Target data (labels). It should correspond to the training data and have the same number of samples.\n",
        "\n",
        "**Q 19 What does model.predict() do? What arguments must be given?**\n",
        "  -  model. predict() is used to generate predictions from the trained model based on new input data.\n",
        "  - The required arguments for model.predict() are:\n",
        "    - It takes input data and corresponding true labels (targets), then calculates the loss and other metrics defined during the model's compilation.\n",
        "    - The function goes through each sample in the dataset, feeds it into the model, and compares the model's predictions to the actual labels to compute the overall performance.\n",
        "\n",
        "**Q 20 What are continuous and categorical variables?**\n",
        "  - continuous variables : continious variables are numerical values with continuous form . like 1,2,3,4,5\n",
        "  - categorical variables : categorical variables are non numerical data grouped into categories or groups. like banana , mango ,apple\n",
        "\n",
        "**Q 21 What is feature scaling? How does it help in Machine Learning?**\n",
        "  - Feature scaling in machine learning is the process of transforming numerical features in a dataset to a common scale or range, ensuring that all features contribute equally to the model's performance.\n",
        "  - How does it help in Machine Learning :-\n",
        "    - Ensures equal contribution\n",
        "    - Improves algorithm performance\n",
        "    - Data compression and storage\n",
        "    \n",
        "**Q 22 How do we perform scaling in Python?**\n",
        "  - we can perform feature scalling by :-\n",
        "1. standardisation :: mu=0 and sigma =1\n",
        "2. normalisation (minmax scalling ):: data comes between 0 and 1\n",
        "3. unit vector >> magnitude is 1\n",
        "  - using sklear.preprocessing StandardScaler\n",
        "  - using sklear.preprocessing MinMaxScaler\n",
        "  - using sklear.preprocessing normalize\n",
        "\n",
        "**Q 23 What is sklearn.preprocessing?**\n",
        "  - - sklearn.preprocessing :- sklearn.preprocessing is a module in the scikit-learn library in Python that provides functions and classes to preprocess data before training machine learning models.\n",
        "  - Preprocessing is a crucial step in machine learning as it transforms raw data into a suitable format that can improve the performance of models. It includes techniques for scaling, normalizing, encoding, and imputing data.\n",
        "\n",
        "**Q 24 How do we split data for model fitting (training and testing) in Python?**\n",
        "  - - In Python, data is typically split for training and testing using the train_test_split function from the scikit-learn library. This function randomly divides the data into two subsets: one for training the model and another for evaluating its performance on unseen data.\n",
        "\n",
        "**Q 25 Explain data encoding?**\n",
        "  - Data encoding is the process of converting data from one format to another, often to make it more suitable for storage, transmission, or processing.\n",
        "  - It involves transforming information into a specific code or format, ensuring it can be read and interpreted by a computer or other system. This process is crucial for various applications, including ensuring data integrity, security, and compatibility between different systems."
      ],
      "metadata": {
        "id": "ejXa_bcIYrZ1"
      }
    }
  ]
}